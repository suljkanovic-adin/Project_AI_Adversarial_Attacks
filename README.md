# Project_AI: On The Empirical Effectiveness of Unrealistic Adversarial Hardening Against Realistic Adversarial Attacks

This repository contains the data and code used in our project titled **"On The Empirical Effectiveness of Unrealistic Adversarial Hardening Against Realistic Adversarial Attacks."** 

## Overview

In this study, we investigate a pivotal question in the field of machine learning security: **Can "cheap" unrealistic adversarial attacks effectively harden machine learning models against more sophisticated and computationally expensive realistic attacks?** 

Our task delves into two distinct use cases:
- **Text Classification**: Assessing the resilience of natural language processing models.
- **Botnet Detection**: Evaluating the robustness of cybersecurity systems against adversarial manipulation.

For each use case, we employ one realistic attack and one unrealistic attack, providing a comprehensive analysis of the effectiveness of adversarial hardening techniques.

## Key Contributions

- **Empirical Analysis**: We present a thorough empirical evaluation of adversarial training methods using both unrealistic and realistic attacks.
- **Comparative Study**: Our findings highlight the differences in model performance when subjected to various attack types.
- **Practical Implications**: The results offer valuable insights for practitioners aiming to enhance model robustness in real-world applications.

## Repository Contents

- **Data**: Datasets used for training and evaluation are obtained via this [link](https://uniluxembourg-my.sharepoint.com/personal/salijona_dyrmishi_uni_lu/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fsalijona%5Fdyrmishi%5Funi%5Flu%2FDocuments%2Frealistic%5Fadversarial%5Fhardening&ga=1)..


## Getting Started


